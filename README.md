# Тренировки по DevOps
Это репозиторий с автоматизационными скриптами по итоговой работе.
Использованы материалы из решения домашней работы по облакам и другим (спасибо им), основа взята из репозитория krya-kryak/y-y-devops-trainings-cloud-1
В проекте участвуют: база данных PostgreSQL, одна облачная виртуалка (coi) с запущенными на ней nginx и двумя экзеплярами bingo, load-balancer.

##  Начальные разбирательства (на локальном компе)
Попробовать начально запустить бинарник решила на домашнем компе. Скачала, сделала запускаемым (`chmod +x bingo`), запустила, почему-то сразу с опцией --help (потом только узнала, что bingo пишет "Hello world", если запустить без опций). Из help понятно стало, что нужна база данных (опция prepare_db), а для базы надо куда-то подключаться, то есть нужен конфиг, решила что надо вывести default конфиг (`./bingo print_default_config`), чтобы понять хотя бы в каком формате он должен быть и что там прописывать. Стало понятно, что нужен PostgreSQL. Но не хотелось PostgreSQL устанавливать в родную операционку, соот-но для простоты перенесла бинарник в виртуальную машину, благо была уже создана у меня виртуалка с установленной Ubuntu. Далее на этой виртуалке устанавила PostgreSQL по инструкции с сайта https://www.postgresql.org/download/linux/ubuntu/, а именно:
`sudo sh -c 'echo "deb https://apt.postgresql.org/pub/repos/apt $(lsb_release -cs)-pgdg main" > /etc/apt/sources.list.d/pgdg.list'`
`wget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add -`
`sudo apt-get update`

БД готова и даже дефолтный конфиг мог бы подойти, если бы не было задания - поменять email. Поэтому записываю на удачу дефолный конфиг в файл `./bingo print_default_config > bingo.conf`, меняю email, пробую вывести текущий конфиг (`./bingo print_current_config`). Но эта команда завершилась ошибкой, поэтому решила попробовать разобраться с strace (`strace ./bingo print_current_config`). И, "бинго" (каламбур) разглядела в выводе вот такую строку: `openat(AT_FDCWD, "/opt/bingo/config.yaml", O_RDONLY|O_CLOEXEC) = -1 ENOENT (No such file or directory)`

Что ж, значит надо перенести конфиг в правильное место с правильным именем файла (`sudo mkdir /opt/bingo; sudo chown $USER /opt/bingo; cp bingo.conf /opt/bingo/config.yaml`). Пробую ещё раз `./bingo print_current_config` и получаю конфиг с моим email. Теперь из списка опций понятно, что надо залить данные в БД (опция prepare_db), запускаю `./bingo prepare_db` жду завершения.

После - пытаюсь запустить сервер `./bingo run_server`, но при запуске опять получаю ошибку, с которой разбираюсь strace-ом и вижу, что bingo пытается получить доступ к файлу `/opt/bongo/logs/ab8c751b93/main.log`, очевидно собирается писать в него логи. Создаю иерархию каталогов, сам файл, делаю $USER владельцем директории `/opt/bingo` и файла.

Запускаю `./bingo run_server`, получаю код "yoohoo_server_launched" (появился вопрос, зачем он нужен). И я не помню где-то тут был момент, что бинго каким-то образом сказал мне, что я использую стандартные креды для подключения к БД и поэтому я создала другого пользователя, создала новую БД, перенесла в неё данные и выдала права на эту БД для нового пользователя, плюс разрешила подключения для него в pg_hba. В общем, сервис запустился, нашла порт на котором он слушает (`ss -ltnp` и нашла незнакомый в списке, или можно `lsof -i -p <pid>`). В ТЗ есть список хендлеров, прошлась по каким-то, вроде норм, работает. Настало время разворачивать в облаке, чтобы отдать на тестирование. Пока думала на эту тему, через некоторое время вернулась к запущенному процессу, а он написал, что panic и завершился. Я попыталась посмотреть/найти какие-то corefile, но ничего не нашлось, запустила процесс заново. Он потом опять упал. Стало понятно, что с этим придётся бороться.

## Запуск в облаке
### Разворачивание PostgreSQL
#### Виртуалка в облаке
1. Создала диск для данных (20Гб - достаточно, его буду монтировать в /var/lib/postgresql)
2. Создала виртуалку (создавала несколько раз, с повышением ресурсов, потому что боролась с непроходящими тестами по отказоустойчивости), в виртуалке подключила диск с данными.
3. Установила PostgreSQL.
4. Создала базу данных, пользователя, выдала права на БД для него, разрешила удалённые подключения (pg_hba).

Далее я попробовала заново запустить prepare_db с запущенной на домашнем компе bingo, но чтобы подкючалась к PostgreSQL в облаке. Но это был оооооочень долго. Поэтому я сделала бекап домашней БД (pg_dump) и восстановила на PostgreSQL (pg_restore).
Проверила, что домашний процесс bingo нормально работает с БД в облаке.

### Разворачивание нод

#### Докер образы. Bingo
Нужен образ для бинго, так как исходников нет, то компилировать его не надо, а надо взять готовый бинарник и упаковать. Добавила bingo в репозиторий с домашкой.
Далее написала "Dockerfile" для сборки: сейчас он лежит в репе с именем `dockerfile-bingo`.
Докерфайл сдалала на основе alpine, потому что в этом образе есть команды для создания директорий, файлов и пользователей (bingo не запускается под root-ом). Это всё нужно, чтобы повторить шаги, которые были сделаны локально. Ну и ещё так как было замечено, что файл с логами увеличивается неадеватно быстро, а полезной информации в нём пока не нашлось, то решила, что будет классно сразу сливать эти логи в /dev/null, чтобы не заморачиваться с какой-нибудь ротацией. Поэтому файл с логами сделала в виде ссылки на /dev/null (`ln -s /dev/null /opt/bongo/logs/ab8c751b93/main.log` в `dockerfile-bingo`). В проде, конечно, надо было бы выкладывать на диск и настраивать ротацию.

#### Докер образы. Nginx
Так как в ТЗ есть пункт про кеширование, то нужен обратный прокси. Выбрала nginx, потому что с ним уже работала.
Сначала создала конфиг (лежит в `config/nginx.conf`) с одним upstream сервером. Позже добавилось кеширование `/long_dummy` (потом ещё  и `/config` закешировала, когда Петя на него поругался), ssl сертификаты для https, rate limit (чтобы проходили тесты на отказоусточивость. Вроде бы из ТЗ в какой-то момент стало понятно, что должно быть).
В докерфайл nginx (`dockerfile-nginx`) добавила копирование конфиг-файла, а позже - генерацию сертификатов. Можно было бы "монтировать" конфиг в контейнер, но пока что так.

#### Стенд в облаке
Теперь надо собрать образы для контейнеров и запустить их. В домашке по "облакам" была как раз такая ситуация, поэтому манифест терраформа взяла из этой домашней работы, как он был. Потому что там всё более-менее подходило.  Разве что имена поменяла chatgpt -> bingo. И подредактировала немного `docker-compose.yaml` и `cloud-config.yaml`. Сейчас в git репе конечно не те файлы, с которыми я запускалась в первый раз :D.
Итак, разворачивание "поэтапное": сначала создаём реестр для образов (`terraform -chdir=terraform apply -target yandex_container_registry.bingo`) в ответ получаем ID реестра, задаём соотв-но переменную (`registry_id=<ID>`) и теперь можем создать и выложить докер образы:
bingo:
`docker build -t cr.yandex/$registry_id/bingo -f dockerfile-bingo .`
`docker push  cr.yandex/$registry_id/bingo`
nginx:
`docker build -t cr.yandex/$registry_id/nginx -f dockerfile-nginx .`
`docker push  cr.yandex/$registry_id/nginx`

После этого можно поднимать виртуалки (`terraform -chdir=terraform apply`) и пытаться подключаться  к сервису. Поднялись мои контейнеры не сразу, конечно. Но поднялись и начали отвечать. К этому моменту "Петя" был готов и я отдала ему на растерзание стенд. И сразу увидела, что мои bingo контейнеры просто выключились. Надо было что-то придумывать, как их поднимать. Создала задание в кроне, чтобы каждую минуту пыталось поднять докер с бинго. Это задание и сейчас сохранилось (в `cloud-config.yaml`). Что ж докеры, как дальше обнаружилось, даже не всегда падают, а ещё умеют переходить в состояние "I feel bad" с 500м статусом ответа. Что ж, написала скрипт, опрашивающий бинго и если 500й статус, то прибивает его (подразумевая что сервис докера поднимет его сам). Этот скрипт немного модифицировался и тоже остался (`/home/ubuntu/bin/checker.py` в `cloud-config.yaml`), но запускается сейчас не в кроне, а как сервис systemd check.service.

К этому моменту, по отчётам Пети стало понятно, что запуск сервиса можно ускорить (у меня был здесь минус). Что ж, вернулась к домашнему запуску бинго (он и правда долго стартовал, секунд 30), опять смотреть strace: в выводе видно какое-то старнное долгое подключение к 8.8.8.8. Вот зачем яндекс сервису лезть в гугл?! Надо отключить эту штуку. Решила забанить 8.8.8.8 при помощи маршрутизации (`ip route add blackhole 8.8.8.8`) и вот ведь, кроме того, что запустился бинго теперь быстро, так ещё и код мне новый выдал "google_dns_is_not_http". Попыталась сдать этот код Пете, оказалось, что в первый раз я код не "отдала" - надо было "плюсик" нажать.

Теперь надо было повторить бан гугла в облачной виртуалке. В тот момент у меня network_mode был = "host", как в домашке, поэтому фокус с баном 8.8.8.8 при помощи маршрутизации провернулся легко. А вот потом, когда я решила запускать на одной виртуалке два бинго, пришлось поменять network_mode на дефолный (бридж), потому что нельзя у бинго просто так поменять порт, тогда пришлось поменять вариант бана с маршрутизации на файрвол (`iptables -I DOCKER-USER -d 8.8.8.8 -j REJECT` в `cloud-config.yaml`).
И примерно в этот момент я случайно обратилась к бинго по корневому хэндлеру (`/`) и получила третий код! "index_page_is_awesome". Тут пришлось взяться и попробовать проверить все ли коды были найдены (`strings bingo | grep "code:"`). Да, кодов - три.
Далее было много-много-много-много попыток запустить стенды в разным количеством виртуалок, с разным ресурсами, чтобы Петя показал все зелёные галочки. Но отказоустойчивость упорно не проходила. Причём то один пункт, то другой, то все три, то ещё какие-то пункты.

А, да ещё пришлось насоздавать много индексов в БД, чтобы не "краснели" хендлеры:
`CREATE INDEX isession_idx ON sessions(id);`
`CREATE INDEX isess_cust_idx ON sessions(customer_id);`
`CREATE INDEX isess_mov_idx ON sessions(movie_id);`
`CREATE INDEX icustomer_idx ON customers(id);`
`CREATE INDEX imovie_idx ON movies(id);`
и даже такие:
`create index cust_surname_idx on customers(surname);`
`create index cust_name_idx on customers(name);`

#### Борьба с отказоустойчивостью
Что делать с отказоустойчивостью было совсем не понятно: я даже переделывала bingo с запуска в докере на запуск через systemd (писала service файл), предполагая, что systemd будет быстрее перезапускать упавший бинго. Но результаты тестов от такого варианта не поменялись.
В итоге, я в чатике начала возмущаться, что вообще невозможно такой сервис поднимать в проде. И после того, как повозмущалась, поняла, что RPS - это не только то, что будет генерировать Петя, но и то, что, пожалуй, надо ограничивать в инсталлированном сервисе. Заодно подумала, что будет лучше если два(или даже больше) бинго  будет крутиться "под крышей" одного общего nginx. В результате размышлений, отказалась от второй виртуалки. Переписала docker-compose и другие конфиги и скрипты.
Далее поискала, как можно в nginx настраивается rate limit. Сначала подумала, что RPS 120 - это вообще для всех-всех запросов, но после такой настройки Петя вообще сказал, что ничего не работает. Я ещё попыталась что-то поделать, ничего не получалось, и подумала, что возможно rps 120 - это для одного IP адреса, переправила. И стало гораздо лучше. И проверки стали проходить почти идеально. На этом я и завершила вносить какие-то улучшения в проект.

#### Не сделано и очень жаль
- Сбор метрик, графики, сбор ошибок
- CI/CD
- https3 (возможно, достаточно было бы понять исходный образ для билда nginx контейнера на уже подготовленный, но доделала это сейчас)
- Разворачивание не одной кнопкой: создаю виртуалку в БД, импортирую данные, создаю реестр, создаю образы, запускаю стенд.
- не делала доменное имя, но это не очень сложно - прописать А-запись в DNS серверах. Но руки не дошли: своё доменное имя даже есть, но сервера не арендованы.
- красиво спрятать логин-пароль подключения к базе и монтирование конфигов к докер-контейнерам
